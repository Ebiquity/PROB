

==== Big Data Processing Workflow ==== 

\only<1>{<<<Images/PipeLine-NoProv.pdf,width=12cm,height=7cm>>>}
\only<2>{<<<Images/PipeLine.pdf,width=12cm,height=7cm>>>}

=! How do we keep track of provenance? !=

\only<2>{<<<Images/lab_notebook1.jpg,width=12cm,height=7cm>>>}
\only<3>{<<<Images/lab_notebook2.jpg,width=12cm,height=7cm>>>}

=! Why these methods don't scale well? !=

* Diligence of the researcher
* Get too overwhelming very quick
* Too subjective

=== ===

\only<1>{<<<Images/StoryInFileNames.png,width=12cm,height=7cm>>>}

=! Extra challenges of big data !=

==== Extra challenges of big data ====

*<1-> Too big for standard version control 
*<2-> Often distribution of datasets is restricted 
*<3-> Jobs run for a long time
*<4-> Exploratory / ad-hoc queries hard to track
